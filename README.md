# lm_retrain
- 언어 모델 재학습 개요
  - BERT, MobileBERT
  - 영화 리뷰 데이터 적용

## 프로그래밍 환경 조성
- (Windows, Linux, Mac) Anaconda + (option) Jupyter notebook
- (IDE) PyCharm

## 왜 언어 모델인가?
- 자연어 처리 분야의 공동 목표 "언어 모델(Language Model)"
  - 사람이 언어를 구사하는 방식을 어떻게 기계로 표현할 것인가?
  - 사람은 언어를 통해 문제를 풀기도 하고, 감정을 드러내기도 하며, 긴 글의 핵심을 요약하고, 특정 언어를 다른 언어로 번역하기도 함
  - 언어 모델은 주어진 단어, 문장, 단락을 토대로 다음에 올 단어를 예측하는 것으로 문맥의 전반적인 이해가 필요
- 임무 지향적 단일 모델 → 언어 모델 기반의 범용 모델
  - 아직까지도 주요 자연어 처리 과업은 단일 모델이 좋은 성능을 보유하고 있으나, 주어진 과업이 아닌 경우 처리가 사실상 불가
  - 언어 모델의 가장 큰 강점은 상대적으로 소수의 학습데이터로 재학습하여 다양한 처리 과업을 수행

## 딥러닝 연구 트렌드의 변화
- 트랜스포머 - 현재의 딥러닝을 지배하는 기술

![image](https://user-images.githubusercontent.com/55765292/197512942-a4ee1386-645a-47e9-b805-8a111140728c.png)

## 언어 모델 재학습
- 심층적인 이론의 이해 없이도 구현 가능
  - 재학습 가능한 언어 모델은 50여개 이상 존재
    - 여기서 재학습이 가능하다는 의미는 언어 모델의 모든 가중치가 공개된 것을 의미
  - huggingface라는 github를 통해 구현의 난도가 현저히 낮아짐
  - 그러나 재학습 결과를 도출하기 위해서는 귀납적 접근이 필수
    - 과업에 따라 큰 모델이 반드시 잘 되리라는 보장이 없음
  - 재학습을 위한 학습데이터를 어떻게, 얼마나 구축할 것인지가 관건
- 재학습용 학습데이터 구축은 결국 도메인 지식
  - 수행하고자 하는 과업에 대한 지식이 가장 중요 → 실제 데이터를 다양하게 보는 관점 확보를 통해 학습데이터의 다양성 확보

### 개괄적 이해
- 준지도학습(Semi Supervised Learning)
  - 비지도학습 + 지도학습의 접근방법
  - 비지도학습 단계(Unsupervised Learning) → 범용 언어 모델 학습
    - 비지도학습의 의미 : 라벨링된 데이터가 필요는 경우에 해당
    - 주어진 텍스트 데이터에서 다음에 올 단어를 빈 칸으로 두고 이를 예측하는 방식으로 학습
    - 다음에 올 단어는 학습데이터에서 활용된 "확률적" 빈도로 결정될 가능성이 높음
    - 학습데이터 예시 : BERT(Wikipedia + Book), GPT-3(Common Crawl + Wikipedia + ...)
  - 지도학습 → 특정 자연어 처리 과업에 재학습
    - Fine-tuning(미세 조정) : 일반적인 언어 모델 재학습
    - 거의 안 해도 된다는 접근 (Zero-shot) : GPT-2
    - 조금만 해도 된다는 접근 (Few-shot) : GPT-3

### Skeleton Code
- Python과 huggingface를 활용한 언어 모델 재학습
  - 가중치를 API를 통해 받거나 huggingface 웹페이지에서 다운받을 수 있음
  - BERT 모델을 기준으로 재학습을 수행하는 소스코드
    - 주석 처리로 MobileBERT 활용 가능
  - 언어 모델에 따라 소스코드 변경이 필요
    - 토큰화 방식, hyperparameter 변경 등
  - 자연어처리 과업에 따라 소스코드 변경 필요
- Skeleton code에서는 영화리뷰 긍부정 데이터 학습
  - 주어진 데이터에서 일부를 1,000건을 학습데이터로 추출하여 재학습
